1-2
1-2-1
ì„ í˜• íšŒê·€ (ê°’ì´ í•˜ë‚˜ë¡œ ê²°ì •ë¨)
ì—¬ëŸ¬ ê°’ë“¤ì´ ìˆëŠ” ê·¸ë˜í”„ë‚˜ í‘œë¥¼ ì§ì„  ìƒìœ¼ë¡œ ë†“ê³  ì˜ˆì¸¡í•˜ëŠ” ê²ƒ
ì˜ˆ- ì§’ê°’) ê¸°ì¡´ ìë£Œë¥¼ í†µí•´ ê°€ê²©ì„ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€
ëŒ€ë‹¤ìˆ˜ëŠ” ë¶„ë¥˜(classifacation)ë¡œ ë¶ˆë¦¼
ì˜ˆë¥¼ ë“¤ì–´ ì•”ì´ ì•…ì„±ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ê²°ì •í•  ë•Œ(ì´ ë•ŒëŠ” 0-1 ì´ì‚° ê°’ì´ë‹¤.)
â€‹
m = í•™ìŠµ ì˜ˆì œì˜ ìˆ˜
x = íŠ¹ì§•(feature)ì´ë¼ ë¶€ë¥´ëŠ” ì…ë ¥ ë³€ìˆ˜ë¥¼ í‘œì‹œí•˜ê¸° ìœ„í•´ ì‚¬ìš©
y = ì¶œë ¥ê°’ì´ë‚˜ ì˜ˆì¸¡í•˜ë ¤ëŠ” ëª©í‘œë³€ìˆ˜
(x,y) = one training example
(x^(i),y(i)) - i th training example
# - training examì˜ ê°œìˆ˜â€‹
â€‹
learning algorithmì€ ê²°ê³¼ê°’ìœ¼ë¡œ ê°„ë‹¤. ë³´í†µ ì†Œë¬¸ì h ë¡œ í‘œí˜„í•¨
â€‹
ì§€ë„ì•Œê³ ë¦¬ì¦˜ì„ ë””ìì¸í•  ë•Œ, ê°€ì„¤ hë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ê²ƒì¸ì§€? ì •í•´ì•¼ í•œë‹¤.
â€‹
â€‹
â€‹
Model Representation
To establish notation for future use, weâ€™ll use <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">x^{(i)}</annotation></semantics></math>x 
(i)
  to denote the â€œinputâ€ variables (living area in this example), also called input features, and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">y^{(i)}</annotation></semantics></math>y 
(i)
  to denote the â€œoutputâ€ or target variable that we are trying to predict (price). A pair <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">(x^{(i)} , y^{(i)} )</annotation></semantics></math>(x 
(i)
 ,y 
(i)
 ) is called a training example, and the dataset that weâ€™ll be using to learnâ€”a list of m training examples <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">{(x^{(i)} , y^{(i)} ); i = 1, . . . , m}</annotation></semantics></math>(x 
(i)
 ,y 
(i)
 );i=1,...,mâ€”is called a training set. Note that the superscript â€œ(i)â€ in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = â„. 

To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X â†’ Y so that h(x) is a â€œgoodâ€ predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:


 When the target variable that weâ€™re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.

1-2-2

ë¹„ìš©í•¨ìˆ˜ : ë°ì´í„°ì— ê°€ì¥ ê°€ê¹Œìš´ ì¼ì°¨í•¨ìˆ˜ ê·¸ë˜í”„ ì•Œì•„ë‚¼ ìˆ˜ ìˆìŒ

ê°€ì„¤í•¨ìˆ˜ : h_c(x)=c_0+c_1

xì„¸íƒ€ê°’ì€ ë§¤ê°œë³€ìˆ˜(parameter)ë¥¼ ë§í•¨.

âˆ‘â€‹ (h(x)-y)^2 ì˜ ê°’ì„ ì¤„ì—¬ë‚˜ê°ˆ ê²ƒì„.

ë¹„ìš©í•¨ìˆ˜ ì‹ : J(ğ›‰_0,ğ›‰_1)=1/2mâˆ‘(h_ğ›‰â€‹(x^(i))-y(i))^2 â€‹

minimize J(ğ›‰_0,ğ›‰â€‹_1)ì´ ëª©ì (ëª©ì í•¨ìˆ˜ë¼ê³ ë„ í•œë‹¤.) (ê°€ë” ì˜¤ì°¨ìš”ì¸ ì œê³±í•¨ìˆ˜ë¼ê³ ë„ ë¶ˆë¦¼)

ì„ í˜• íšŒê·€ì—ì„œ ì´ ë¹„ìš©í•¨ìˆ˜(ëª©ì í•¨ìˆ˜)ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ëª©ì 

1-2-3

ë¹„ìš©í•¨ìˆ˜ì˜ ì˜ˆì‹œë¥¼ í†µí•´ ë¹„ìš©í•¨ìˆ˜ê°€ í•˜ëŠ” ì¼ & ì™œ ì‚¬ìš©í•˜ëŠ”ì§€

1-2-4

ì´ë²ˆ ë¹„ë””ì˜¤ëŠ” ê± ê±´ë„ˆ ë›°ì–´ë„ ëœëŒ€ í—¤í—¤ í¸í•˜ê²Œ ë´ì•¼ì§€

â€‹

â€‹
