1-2
1-2-1
선형 회귀 (값이 하나로 결정됨)
여러 값들이 있는 그래프나 표를 직선 상으로 놓고 예측하는 것
예- 짒값) 기존 자료를 통해 가격을 예측하는 회귀
대다수는 분류(classifacation)로 불림
예를 들어 암이 악성인지 아닌지를 결정할 때(이 때는 0-1 이산 값이다.)
​
m = 학습 예제의 수
x = 특징(feature)이라 부르는 입력 변수를 표시하기 위해 사용
y = 출력값이나 예측하려는 목표변수
(x,y) = one training example
(x^(i),y(i)) - i th training example
# - training exam의 개수​
​
learning algorithm은 결과값으로 간다. 보통 소문자 h 로 표현함
​
지도알고리즘을 디자인할 때, 가설 h를 어떻게 표현할 것인지? 정해야 한다.
​
​
​
Model Representation
To establish notation for future use, we’ll use <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">x^{(i)}</annotation></semantics></math>x 
(i)
  to denote the “input” variables (living area in this example), also called input features, and <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">y^{(i)}</annotation></semantics></math>y 
(i)
  to denote the “output” or target variable that we are trying to predict (price). A pair <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">(x^{(i)} , y^{(i)} )</annotation></semantics></math>(x 
(i)
 ,y 
(i)
 ) is called a training example, and the dataset that we’ll be using to learn—a list of m training examples <math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">{(x^{(i)} , y^{(i)} ); i = 1, . . . , m}</annotation></semantics></math>(x 
(i)
 ,y 
(i)
 );i=1,...,m—is called a training set. Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ. 

To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this:


 When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem. When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem.

1-2-2

비용함수 : 데이터에 가장 가까운 일차함수 그래프 알아낼 수 있음

가설함수 : h_c(x)=c_0+c_1

x세타값은 매개변수(parameter)를 말함.

∑​ (h(x)-y)^2 의 값을 줄여나갈 것임.

비용함수 식 : J(𝛉_0,𝛉_1)=1/2m∑(h_𝛉​(x^(i))-y(i))^2 ​

minimize J(𝛉_0,𝛉​_1)이 목적(목적함수라고도 한다.) (가끔 오차요인 제곱함수라고도 불림)

선형 회귀에서 이 비용함수(목적함수)를 구하는 것이 목적

1-2-3

비용함수의 예시를 통해 비용함수가 하는 일 & 왜 사용하는지

1-2-4

이번 비디오는 걍 건너 뛰어도 된대 헤헤 편하게 봐야지

​

​
